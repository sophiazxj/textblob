{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob: Simplified Text Processing\n",
    "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "**TextBlob** stands on the giant shoulders of **NLTK** and **pattern**, and plays nicely with both.\n",
    "\n",
    "### Features\n",
    "* Noun phrase extraction\n",
    "* Part-of-speech tagging\n",
    "* Sentiment analysis\n",
    "* Classification (Naive Bayes, Decision Tree)\n",
    "* Language translation and detection powered by Google Translate\n",
    "* Tokenization (splitting text into words and sentences)\n",
    "* Word and phrase frequencies\n",
    "* Parsing\n",
    "* n-grams\n",
    "* Word inflection (pluralization and singularization) and lemmatization\n",
    "* Spelling correction\n",
    "* Add new models or languages through extensions\n",
    "* WordNet integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('titular', 'JJ'),\n",
       " ('threat', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('Blob', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('always', 'RB'),\n",
       " ('struck', 'VBN'),\n",
       " ('me', 'PRP'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('ultimate', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('monster', 'NN'),\n",
       " ('an', 'DT'),\n",
       " ('insatiably', 'RB'),\n",
       " ('hungry', 'JJ'),\n",
       " ('amoeba-like', 'JJ'),\n",
       " ('mass', 'NN'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('penetrate', 'VB'),\n",
       " ('virtually', 'RB'),\n",
       " ('any', 'DT'),\n",
       " ('safeguard', 'NN'),\n",
       " ('capable', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('doomed', 'JJ'),\n",
       " ('doctor', 'NN'),\n",
       " ('chillingly', 'RB'),\n",
       " ('describes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('assimilating', 'VBG'),\n",
       " ('flesh', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('contact', 'NN'),\n",
       " ('Snide', 'JJ'),\n",
       " ('comparisons', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('gelatin', 'VB'),\n",
       " ('be', 'VB'),\n",
       " ('damned', 'VBN'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('concept', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('devastating', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('potential', 'JJ'),\n",
       " ('consequences', 'NNS'),\n",
       " ('not', 'RB'),\n",
       " ('unlike', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('grey', 'NN'),\n",
       " ('goo', 'NN'),\n",
       " ('scenario', 'NN'),\n",
       " ('proposed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('technological', 'JJ'),\n",
       " ('theorists', 'NNS'),\n",
       " ('fearful', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('artificial', 'JJ'),\n",
       " ('intelligence', 'NN'),\n",
       " ('run', 'NN'),\n",
       " ('rampant', 'NN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a textblob\n",
    "blob = TextBlob(text)\n",
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['titular threat', 'blob', 'ultimate movie monster', 'amoeba-like mass', 'snide', 'potential consequences', 'grey goo scenario', 'technological theorists fearful', 'artificial intelligence run rampant'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06000000000000001\n",
      "-0.34166666666666673\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Blob的名义威胁一直让我成为最终的电影\n",
       "怪物：一个不能容忍的饥饿，阿米巴样的物质能够穿透\n",
       "几乎所有的保障措施，都能成为一个悲观的医生\n",
       "描述它 - “同化接触肉体。\n",
       "Snide比较明胶是该死的，这是一个最多的概念\n",
       "破坏性的潜在后果，不像灰色的情况\n",
       "技术理论家提出的可怕的提议\n",
       "人工智能猖獗。\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.translate(to = 'zh-CN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Language Support for the Phrase-Based Machine Translation Model**\n",
    "\n",
    "Language|ISO-639-1 Code|Language|ISO-639-1 Code\n",
    "--------|--------------|--------|--------------\n",
    "Afrikaans|af|Latin|la\n",
    "Albanian|sq|Latvian|lv\n",
    "Amharic|am|Lithuanian|lt\n",
    "Arabic|ar|Luxembourgish|lb\n",
    "Armenian|hy|Macedonian|mk\n",
    "Azeerbaijani|az|Malagasy|mg\n",
    "Basque|eu|Malay|ms\n",
    "Belarusian|be|Malayalam|ml\n",
    "Bengali|bn|Maltese|mt\n",
    "Bosnian|bs|Maori|mi\n",
    "Bulgarian|bg|Marathi|mr\n",
    "Catalan|ca|Mongolian|mn\n",
    "Cebuano|ceb (ISO-639-2)|Myanmar (Burmese)|my\n",
    "Chinese (Simplified)|zh-CN (BCP-47)|Nepali|ne\n",
    "Chinese (Traditional)|zh-TW (BCP-47)|Norwegian|no\n",
    "Corsican|co|Nyanja (Chichewa)|ny\n",
    "Croatian|hr|Pashto|ps\n",
    "Czech|cs|Persian|fa\n",
    "Danish|da|Polish|pl\n",
    "Dutch|nl|Portuguese (Portugal, Brazil)|pt\n",
    "English|en|Punjabi|pa\n",
    "Esperanto|eo|Romanian|ro\n",
    "Estonian|et|Russian|ru\n",
    "Finnish|fi|Samoan|sm\n",
    "French|fr|Scots Gaelic|gd\n",
    "Frisian|fy|Serbian|sr\n",
    "Galician|gl|Sesotho|st\n",
    "Georgian|ka|Shona|sn\n",
    "German|de|Sindhi|sd\n",
    "Greek|el|Sinhala (Sinhalese)|si\n",
    "Gujarati|gu|Slovak|sk\n",
    "Haitian Creole|ht|Slovenian|sl\n",
    "Hausa|ha|Somali|so\n",
    "Hawaiian|haw (ISO-639-2)|Spanish|es\n",
    "Hebrew|iw|Sundanese|su\n",
    "Hindi|hi|Swahili|sw\n",
    "Hmong|hmn (ISO-639-2)|Swedish|sv\n",
    "Hungarian|hu|Tagalog (Filipino)|tl\n",
    "Icelandic|is|Tajik|tg\n",
    "Igbo|ig|Tamil|ta\n",
    "Indonesian|id|Telugu|te\n",
    "Irish|ga|Thai|th\n",
    "Italian|it|Turkish|tr\n",
    "Japanese|ja|Ukrainian|uk\n",
    "Javanese|jw|Urdu|ur\n",
    "Kannada|kn|Uzbek|uz\n",
    "Kazakh|kk|Vietnamese|vi\n",
    "Khmer|km|Welsh|cy\n",
    "Korean|ko|Xhosa|xh\n",
    "Kurdish|ku|Yiddish|yi\n",
    "Kyrgyz|ky|Yoruba|yo\n",
    "Lao|lo|Zulu|zu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tesrimonial = TextBlob('Textblob is amazingly simple to use. What great fun!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesrimonial.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We can break TextBlobs into words or sentences\n",
    "\n",
    "Sentence objects have the same properties and methods as TextBlobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zen = TextBlob(\"Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Beautiful', 'is', 'better', 'than', 'ugly', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'beautiful': 1,\n",
       "             'better': 3,\n",
       "             'complex': 1,\n",
       "             'explicit': 1,\n",
       "             'implicit': 1,\n",
       "             'is': 3,\n",
       "             'simple': 1,\n",
       "             'than': 3,\n",
       "             'ugly': 1})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Beautiful is better than ugly.\"),\n",
       " Sentence(\"Explicit is better than implicit.\"),\n",
       " Sentence(\"Simple is better than complex.\")]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.2166666666666667, subjectivity=0.8333333333333334)\n",
      "Sentiment(polarity=0.5, subjectivity=0.5)\n",
      "Sentiment(polarity=0.06666666666666667, subjectivity=0.41904761904761906)\n"
     ]
    }
   ],
   "source": [
    "for s in zen.sentences:\n",
    "    print(s.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Inflection and  Lemmatization (词变形和词形归并)\n",
    "\n",
    "Each word in **TextBlob.words** or **Sentence.words** is a **Word object** (a subclass of unicode) with useful methods, e.g. for __word inflection__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sentence = TextBlob(\"Use 4 spaces per indentation level.\")\n",
    "sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将第三个单词spaces转换为单数形式\n",
    "sentence.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'levels'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将句子中最后一个单词转换为复数形式.\n",
    "sentence.words[-1].pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "w =Word('octopi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopus'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('went')\n",
    "w.lemmatize('v') #pass in WordNet part of speech (verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet Integration\n",
    "\n",
    "You can access the **synsets**(同义词集) for a Word via the synsets property or the **get_synsets** method, optionally passing in a part of speech.\n",
    "\n",
    "You can access the **definitions** for each synset via the **definitions** property or the **define()** method, which can also take an optional part-of-speech argument.\n",
    "\n",
    "For more info. see the NLTK documentation on [Wordnet Interface](http://www.nltk.org/howto/wordnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('octopus.n.01'), Synset('octopus.n.02')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "\n",
    "word = Word(\"octopus\")\n",
    "word.synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chop.v.05'),\n",
       " Synset('hack.v.02'),\n",
       " Synset('hack.v.03'),\n",
       " Synset('hack.v.04'),\n",
       " Synset('hack.v.05'),\n",
       " Synset('hack.v.06'),\n",
       " Synset('hack.v.07'),\n",
       " Synset('hack.v.08')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word('hack').get_synsets(pos = VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tentacles of octopus prepared as food',\n",
       " 'bottom-living cephalopod having a soft oval body with eight long tentacles']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word('octopus').definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Creat synsets directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.wordnet import Synset\n",
    "octopus = Synset(\"octopus.n.02\")\n",
    "shrimp = Synset(\"shrimp.n.03\")\n",
    "octopus.path_similarity(shrimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordLists\n",
    "\n",
    "A **WordList** is just a Python list with additional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cat', 'dog', 'octopus'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = TextBlob(\"cat dog octopus\")\n",
    "animals.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cats', 'dogs', 'octopodes'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.words.pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "\n",
    "Use the **corrrect()** method to attempt spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"I havv goood speling!\")\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word** object have a ** spellchech() Word.spellcheck() ** method that returns a list of (word, confidence) tuples with spelling suggestions.\n",
    "\n",
    "Spelling correction is based on Peter Norvig’s “How to Write a Spelling Corrector”[1] as implemented in the pattern library. It is about 70% accurate [[2]](https://textblob.readthedocs.io/en/dev/quickstart.html#id5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 1.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word(\"falibility\")\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word and Noun Phrase Frequencies\n",
    "There are two ways to get the frequency of a word or noun phrase in a TextBlob.\n",
    "\n",
    "* <span style=\"color:red\">First is through the word_counts dictionary.</span>\n",
    "* Second way is to use the count() method.\n",
    "\n",
    "<span style=\"color:red\">I tried and find that the first method does not work.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = TextBlob(\"We are no longer the Knights who say Ni. We are now the Knights who say Ekki ekki ekki PTANG.\")\n",
    "monty.words.count('ekki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify whether it's case sensitive.\n",
    "monty.words.count('ekki', case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['ni', 'ekki', 'ekki ekki', 'ptang'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Usage: Overriding Models and the Blobber Class\n",
    "TextBlob允许指定想用的算法.\n",
    "\n",
    "The **textblob.sentiments** module contains two sentiment analysis implementations, **PatternAnalyzer** (based on the pattern library) and **NaiveBayesAnalyzer** (an NLTK classifier trained on a movie reviews corpus).\n",
    "\n",
    "The default implementation is **PatternAnalyzer**, but you can override the analyzer by passing another implementation into a TextBlob’s constructor.\n",
    "\n",
    "For instance, the **NaiveBayesAnalyzer** returns its result as a namedtuple of the form: **Sentiment(classification, p_pos, p_neg)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.7996209910191279, p_neg=0.2003790089808724)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"I love this library\", analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "The **words** and **sentences** properties are helpers that use the **textblob.tokenizers.WordTokenizer** and **textblob.tokenizers.SentenceTokenizer** classes, respectively.\n",
    "\n",
    "You can use other tokenizers, such as those provided by **NLTK**, by passing them into the **TextBlob constructor** then accessing the tokens property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['This is', 'a rather tabby', 'blob.'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TabTokenizer\n",
    "tokenizer = TabTokenizer()\n",
    "blob = TextBlob('This is\\ta rather tabby\\tblob.', tokenizer=tokenizer)\n",
    "blob.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to use the **tokenize([tokenizer])** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['A token', 'of appreciation'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "tokenizer = BlanklineTokenizer()\n",
    "blob = TextBlob(\"A token\\n\\nof appreciation\")\n",
    "blob.tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrase Chunkers\n",
    "TextBlob currently has two noun phrases chunker implementations, **textblob.np_extractors.FastNPExtractor** (default, based on Shlomi Babluki’s implementation from this blog post) and **textblob.np_extractors.ConllExtractor**, which uses the CoNLL 2000 corpus to train a tagger.\n",
    "\n",
    "Use **np_extractor** to explicitly passing an instance of a noun phrase extractor to a TextBlob's constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['python', 'high-level programming language'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.np_extractors import ConllExtractor\n",
    "extractor = ConllExtractor()\n",
    "blob = TextBlob(\"Python is a high-level programming language.\",np_extractor=extractor)\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Taggers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
